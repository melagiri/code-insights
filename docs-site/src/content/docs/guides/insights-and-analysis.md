---
title: Insights & Analysis
description: Generate AI-powered insights from your sessions — summaries, decisions, learnings, techniques, and prompt quality scores.
---

Insights are the intelligence layer of Code Insights. Raw session transcripts become structured knowledge — decisions you made, things you learned, patterns worth repeating, and prompt habits to improve. Instead of scrolling through hundreds of messages to remember what happened last Tuesday, you get a set of typed, searchable artifacts that capture the substance of each session. All analysis runs client-side using your own API keys — your data never touches an intermediary server.

## How Analysis Works

The analysis pipeline is straightforward:

1. You provide an API key for your chosen LLM provider (OpenAI, Anthropic, Gemini, or Ollama for fully local inference).
2. When you click **Analyze** on a session, the dashboard collects the session's messages and sends them directly to the LLM.
3. The LLM returns structured insights — summaries, decisions, learnings, techniques, or prompt quality assessments — formatted according to a system prompt that enforces consistent output.
4. The dashboard saves these insights to the `insights` collection in your Firestore, where they become browsable, filterable, and permanently linked to the session that produced them.

:::note
**Privacy:** Your session data travels directly from your browser to the LLM provider's API. There is no intermediary server, no proxy, and no telemetry. The dashboard is a static client — it reads from your Firestore and calls the LLM on your behalf, and that's it.
:::

## Configuring Your LLM Provider

Before you can run analysis, you need to tell the dashboard which LLM to use.

1. Click the **Settings** icon (gear, top-right corner of the dashboard).
2. In the **LLM Provider** section, select a provider from the dropdown.
3. Enter your API key in the field provided.
4. Choose a model from the available options — each provider offers multiple models with different speed, quality, and cost profiles.

| Provider | Popular Models | Key Required |
|----------|---------------|--------------|
| OpenAI | gpt-4o, gpt-4o-mini | Yes |
| Anthropic | claude-sonnet-4, claude-3.5-haiku, claude-opus-4 | Yes |
| Gemini | gemini-2.0-flash, gemini-1.5-pro | Yes |
| Ollama | Any local model | No (runs locally) |

Your provider configuration is saved to `localStorage` (key: `code_insights_llm_config`), so it persists across browser sessions without ever leaving your machine.

:::tip
For the best balance of quality and cost, **gpt-4o** and **gemini-2.0-flash** are strong defaults. If you want the highest-quality analysis and don't mind the cost, **claude-opus-4** or **gpt-4o** produce the most nuanced insights. For local inference with zero API costs, install [Ollama](https://ollama.com) and point it at a capable model.
:::

:::caution
The Anthropic provider runs with the `anthropic-dangerous-direct-browser-access` header enabled, since the dashboard is a browser client calling the Anthropic API directly. This is by design — there's no backend proxy. Be aware that your Anthropic API key is used from the browser, so treat it accordingly.
:::

Token input is capped at **80,000 tokens** per request to stay within model context limits. For sessions that exceed this cap, the dashboard automatically chunks the content across multiple requests.

## Running Analysis

There are two ways to generate insights.

### Single Session Analysis

Open any session detail page. Click the **Analyze** dropdown button and choose the type of analysis you want:

- **Full Analysis** — Generates all insight types at once (summary, decisions, learnings, techniques)
- **Prompt Quality** — Runs a dedicated prompt efficiency analysis with scoring and anti-pattern detection

The analysis runs in your browser, and results appear on the page as soon as the LLM responds.

### Bulk Analysis

On the dashboard home page, if you have sessions that haven't been analyzed yet, you'll see an **"Analyze N Sessions"** button. Clicking it queues up analysis for all unanalyzed sessions, processing them sequentially. This is the fastest way to catch up after syncing a batch of new sessions.

:::tip
Bulk analysis can take a few minutes if you have many sessions queued. The dashboard shows progress as each session completes. You can continue browsing while it runs.
:::

## Five Insight Types

Every insight generated by Code Insights falls into one of five types. Each type extracts a different dimension of knowledge from your session.

### Summary

A narrative overview of what was accomplished during the session. Summary insights distill an entire conversation — sometimes hundreds of messages — into a concise description with supporting bullet points.

A summary card might read: **"Pre-launch audits, fixes, and docs site deployment"** with bullets like:
- Web PR #42 merged: ARIA labels, error.tsx, dynamic version
- Docs live at docs.code-insights.app with custom domain
- CLI v0.6.2 published with --regenerate-titles flag

Summaries are the insight type you'll reference most often — they're the "what happened" record for every session.

### Decisions

Architectural and design choices captured with full context. Each decision card shows the choice that was made, the alternatives that were considered, and the reasoning behind the final call.

For example: **"Starlight chosen over Docusaurus/VitePress for docs"** with alternatives:
- Docusaurus (63k stars, mature ecosystem, but heavier bundle)
- VitePress (17k stars, Vue-based, less Astro-native)
- Mintlify (SaaS, $30/mo, no self-hosting)

And reasoning: "Starlight integrates natively with the existing Astro ecosystem, supports MDX out of the box, and has zero runtime cost."

Decision insights are valuable weeks or months later, when you've forgotten *why* you chose one approach over another. They're your architectural decision records, generated automatically.

### Learnings

Transferable knowledge extracted from the conversation, with evidence citations pointing back to specific message numbers in the transcript.

For example: **"Vercel root directory config for monorepo deploys"** — explains that deploying a subdirectory from a monorepo requires setting the Root Directory in Vercel project settings, not just changing the build command. The insight cites User #54 and Assistant #116 as the source messages, so you can jump back to the original conversation for full context.

Learnings capture the "aha" moments — things you figured out during the session that apply beyond just this project.

### Techniques

Problem-solving approaches and debugging strategies that are reusable across contexts. Technique insights include applicability context so you know *when* the approach is relevant.

For example: **"Binary search debugging for CSS layout issues"** — describes isolating a layout bug by commenting out half the styles, then narrowing from there. Applicability: "When a CSS change breaks layout and you can't identify which rule is responsible from inspection alone."

Techniques are the "how to" counterpart to learnings' "what I discovered."

### Prompt Quality

An efficiency score from 0 to 100 with a detailed breakdown of your prompting habits during the session. This is the most actionable insight type — it shows you exactly where you wasted turns and how to write better prompts. See the next section for a full breakdown.

## Prompt Quality Deep Dive

Prompt quality analysis evaluates how efficiently you communicated with Claude during a session. It's not about whether the session was productive — it's about whether you could have reached the same outcome with fewer messages and less friction.

### Score

Every prompt quality insight includes a score from **0 to 100** with a rating label:

- **80-100: Excellent** — Clear, well-scoped prompts with good context
- **60-79: Good** — Generally effective with some room for improvement
- **Below 60: Needs Improvement** — Significant wasted turns or unclear communication

### Description

An LLM-written narrative assessment of your overall prompting efficiency for the session. This reads like a brief review: what you did well, where you lost time, and the general pattern of communication.

### Message Reduction Estimate

One of the most concrete outputs: **"Could have been done in N fewer messages."** This quantifies the waste — if you sent 40 messages but could have accomplished the same work in 28, the analysis will call out those 12 unnecessary round trips.

### Anti-Patterns Detected

The analysis identifies recurring prompting anti-patterns and counts how many times each appeared. Common patterns include:

- **Missing Context** — e.g., pasting "Tool results: Exit code 1" without specifying which tool failed or what the error output said
- **Unclear Intent** — e.g., "yes, tackle all of it" without specifying which items from a list of options
- **Rushed Input / Typos** — e.g., "whgo ahead" or "do the thign" — not harmful individually, but indicative of insufficient review before sending
- **Assumption of Task State Without Verification** — e.g., asking Claude to fix something that was already fixed in a previous commit without checking first

### Wasted Turns

Specific message references with explanations of what went wrong. For example:

> **Msg #6:** Task #2 was already completed via PR #37 — user could have verified the PR status before requesting work that was already done.

Each wasted turn includes an expandable **"Better prompt"** suggestion showing how the message could have been rewritten to avoid the wasted exchange.

### Tips

Actionable improvement advice tailored to the session's specific patterns. These are concrete, not generic — instead of "be more specific," you'll see tips like "Include the file path when referencing errors, since the same error message can appear in different files" or "Check `git status` before asking Claude to commit — three turns were spent resolving uncommitted changes."

![Prompt Quality Analysis showing score, anti-patterns, and tips](/images/prompt-quality.png)

## Browsing Insights

The **Insights** page (accessible from the sidebar) provides a scrollable feed of all insights across all your sessions and projects.

Each insight card displays:

- **Type badge** — Color-coded by type: green for Learning, blue for Decision, purple for Summary, orange for Technique, red/pink for Prompt Quality
- **Title** — The insight's headline
- **Project name** — Which project the session belonged to
- **Content preview** — A brief excerpt of the insight body
- **Timestamp** — When the source session occurred

You can **filter by project** using the dropdown at the top of the page to focus on insights from a specific codebase.

Clicking any insight card takes you to the session that generated it, so you can read the full transcript for additional context.

![Insights browse page with Learning, Decision, and Summary cards](/images/insight-cards.png)

:::tip
The Insights page is particularly useful for onboarding. If you're returning to a project after a break, filtering insights to that project gives you a structured recap — what decisions were made, what techniques were used, and what was learned — without re-reading every session transcript.
:::
